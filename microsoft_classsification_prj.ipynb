{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7aWQqN63XJo"
      },
      "source": [
        "# Microsoft Classification Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT019qFY3fRF"
      },
      "source": [
        "Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i9NSWveMBcbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751ab520-908f-4dba-81c7-d8530d126c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category-encoders\n",
            "  Downloading category_encoders-2.8.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (2.2.2)\n",
            "Collecting patsy>=0.5.1 (from category-encoders)\n",
            "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders) (1.13.1)\n",
            "Collecting statsmodels>=0.9.0 (from category-encoders)\n",
            "  Downloading statsmodels-0.14.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category-encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category-encoders) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category-encoders) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category-encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category-encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category-encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category-encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.9/232.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsmodels-0.14.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patsy, statsmodels, category-encoders\n",
            "Successfully installed category-encoders-2.8.0 patsy-1.0.1 statsmodels-0.14.4\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl (223.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.25.1 xgboost-2.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install category-encoders\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UWTGUER7Blwf"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries for this project\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import category_encoders as ce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thF34CpB3qVU"
      },
      "source": [
        "**## 1. Reading input Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ll1PXQ4-BqI2"
      },
      "outputs": [],
      "source": [
        "def read_data_custom():\n",
        "  # Reading from Goggle drive as running millions of records in local is not possible\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  # Reading data from drive\n",
        "\n",
        "  # Reading 0.01 percent of data inorder to make this test with smaller data before moving to whole data\n",
        "  #nd_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_data/GUIDE_Train.csv', skiprows=lambda i: i>0 and np.random.rand() > 0.01)\n",
        "\n",
        "  # Once everything done, comment above line and run below\n",
        "  nd_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_data/GUIDE_Train.csv')\n",
        "  return nd_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJSRvXlj30MX"
      },
      "source": [
        "# 2. Data Exploration and feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b_GcC780B4Zt"
      },
      "outputs": [],
      "source": [
        "def data_quality(df):\n",
        "  df = df.copy()\n",
        "  # Drop duplicate columns\n",
        "  df = df.drop_duplicates(keep=False)\n",
        "  print(df.shape)\n",
        "  #Dropping ID columns as it will not help in machine learning prediction\n",
        "  id_columns = ['Id', 'OrgId', 'IncidentId', 'AlertId', 'DetectorId', 'DeviceId']\n",
        "  df.drop(columns=id_columns)\n",
        "\n",
        "  df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "\n",
        "  df['day_of_week'] = df['Timestamp'].dt.dayofweek\n",
        "\n",
        "  #dropping timestamp column\n",
        "  df.drop('Timestamp', axis=1, inplace=True)\n",
        "\n",
        "  df['day_of_week'] = df['day_of_week'].astype('int64')\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETpakERt4C8s"
      },
      "source": [
        "# 3. Data Cleaning function for train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TE1GUo_JB4b2"
      },
      "outputs": [],
      "source": [
        "def data_cleaning(df):\n",
        "  df[\"EmailClusterId\"] = df[\"EmailClusterId\"].fillna(-1)\n",
        "  #Dropping majority of missing columns as it will have lesser impact on final prediction\n",
        "  major_null_columns = ['ActionGrouped', 'ActionGranular', 'ThreatFamily', 'ResourceType', 'Roles', 'AntispamDirection']\n",
        "  df.drop(columns=major_null_columns, inplace=True)\n",
        "\n",
        "  df[['MitreTechniques', 'SuspicionLevel', 'LastVerdict']] = df[['MitreTechniques', 'SuspicionLevel', 'LastVerdict']].fillna('Unknown')\n",
        "\n",
        "  # Filling the EmailClusterId with the most frequent values in list\n",
        "  df['EmailClusterId'] = df['EmailClusterId'].fillna(df['EmailClusterId'].mode()[0])\n",
        "\n",
        "  df = df.dropna(subset=['IncidentGrade']).copy()\n",
        "  label_encoder = LabelEncoder()\n",
        "  df['EncodedIncidentGrade'] = label_encoder.fit_transform(df['IncidentGrade'])\n",
        "  df.drop(columns='IncidentGrade', inplace=True)\n",
        "\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IunMIZc_4MuP"
      },
      "source": [
        "# 4. Data Preprocessing function for train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NltdQcmxB4eV"
      },
      "outputs": [],
      "source": [
        "def encode_categorical_values(df):\n",
        "  label_encoder = LabelEncoder()\n",
        "  df['EncodedCategory'] = label_encoder.fit_transform(df['Category'].fillna('Unknown'))  # Handling NaN\n",
        "\n",
        "  # Encoding 'MitreTechniques' using Target Encoding (Mean Encoding)\n",
        "  target_encoder = ce.TargetEncoder(cols=['MitreTechniques'])\n",
        "  df['EncodedMitreTechniques'] = target_encoder.fit_transform(df['MitreTechniques'], df['EncodedCategory'])\n",
        "\n",
        "  # One-Hot Encoding for 'EntityType'\n",
        "  df = pd.get_dummies(df, columns=['EntityType'], drop_first=True)\n",
        "\n",
        "  # Encoding 'EvidenceRole', 'SuspicionLevel', and 'LastVerdict' using Label Encoding\n",
        "  df['EncodedEvidenceRole'] = label_encoder.fit_transform(df['EvidenceRole'])\n",
        "  df['EncodedSuspicionLevel'] = label_encoder.fit_transform(df['SuspicionLevel'])\n",
        "  df['EncodedLastVerdict'] = label_encoder.fit_transform(df['LastVerdict'])\n",
        "\n",
        "  # Step 2: Dropping Redundant Columns (Original Categorical Columns)\n",
        "  df = df.drop(columns=['Category', 'MitreTechniques', 'EvidenceRole', 'SuspicionLevel', 'LastVerdict'])\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpjqc0JJ4Vvn"
      },
      "source": [
        "# 5. Train Test Split function for train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gTKG2viCB4h6"
      },
      "outputs": [],
      "source": [
        "def train_test_split_custom(df):\n",
        "  X = df.drop('EncodedIncidentGrade', axis=1)  # Drop the target column to get features\n",
        "  y = df['EncodedIncidentGrade']  # The target variable\n",
        "  # Split the data into training and test sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  return X_train, X_test, y_train, y_test, X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb5B46YA4crq"
      },
      "source": [
        "# 6. Logistic Regression Model for train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dCRRzg_3CAbh"
      },
      "outputs": [],
      "source": [
        "def logistic_regression_model(X_train, X_test, y_train, y_test):\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "  # Save the fitted scaler\n",
        "  with open(\"log_scaler.pkl\", \"wb\") as scaler_file:\n",
        "      pickle.dump(scaler, scaler_file)\n",
        "\n",
        "  # Initialize the Logistic Regression model\n",
        "  log_reg = LogisticRegression(max_iter=1000)\n",
        "  # Train the model\n",
        "  log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "  # Detailed classification report\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  with open(\"logistic_regression_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(log_reg, model_file)\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLCYXTOv4jqx"
      },
      "source": [
        "# 7. Xgboost model function for Tran data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iRt7r3hqCCgh"
      },
      "outputs": [],
      "source": [
        "def xgboost_model(X_train, X_test, y_train, y_test, y):\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "  xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(y.unique()), eval_metric='mlogloss')\n",
        "  xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "  y_pred = xgb_model.predict(X_test_scaled)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Accuracy: {accuracy:.4f}\")\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  with open(\"xgboost_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(xgb_model, model_file)\n",
        "    # Save the fitted scaler\n",
        "  with open(\"xgb_scaler.pkl\", \"wb\") as scaler_file:\n",
        "      pickle.dump(scaler, scaler_file)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZR828ej4rSi"
      },
      "source": [
        "# 8. Random Forest Model for Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KcZnVnVACE28"
      },
      "outputs": [],
      "source": [
        "def random_forest_model(X_train, X_test, y_train, y_test):\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "  rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "  rf_model.fit(X_train_scaled, y_train)\n",
        "  y_pred = rf_model.predict(X_test_scaled)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "  # Detailed classification report\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  with open(\"random_forest_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(rf_model, model_file)\n",
        "\n",
        "  with open(\"rf_scaler.pkl\", \"wb\") as scaler_file:\n",
        "      pickle.dump(scaler, scaler_file)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2FNBqqY41dA"
      },
      "source": [
        "# 9. KFold Cross Validation for train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qjth1JXHCr92"
      },
      "outputs": [],
      "source": [
        "\n",
        "def kfold_xgboost(X, y, k=5):\n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "    accuracies = []\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # XGBoost model\n",
        "        xgb_model = xgb.XGBClassifier(objective='multi:softmax',\n",
        "                                      num_class=len(y.unique()),\n",
        "                                      eval_metric='mlogloss')\n",
        "        xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = xgb_model.predict(X_test_scaled)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "        print(f\"Fold Accuracy: {acc:.4f}\")\n",
        "        print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    print(f\"\\nAverage Accuracy across {k} folds: {np.mean(accuracies):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzgbw3R347fJ"
      },
      "source": [
        "# Execution all the function to get prediction score of models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyjqfPr8CHFK",
        "outputId": "74c41675-7ea5-4d5b-9777-23206bcdc020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "df = read_data_custom()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbF-RkCOCOOX",
        "outputId": "010ad760-b5c7-4f03-8f85-fbdf9b2f1bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9481718, 45)\n"
          ]
        }
      ],
      "source": [
        "df = data_quality(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HqMiL5m-CgZn"
      },
      "outputs": [],
      "source": [
        "df = data_cleaning(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kIGdhLGiCh_Q"
      },
      "outputs": [],
      "source": [
        "df = encode_categorical_values(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvuBkBl5_5Ra",
        "outputId": "f7c8dff0-553c-46cb-dbf6-d3ed943b541c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Id', 'OrgId', 'IncidentId', 'AlertId', 'DetectorId', 'AlertTitle',\n",
            "       'DeviceId', 'Sha256', 'IpAddress', 'Url', 'AccountSid', 'AccountUpn',\n",
            "       'AccountObjectId', 'AccountName', 'DeviceName', 'NetworkMessageId',\n",
            "       'EmailClusterId', 'RegistryKey', 'RegistryValueName',\n",
            "       'RegistryValueData', 'ApplicationId', 'ApplicationName',\n",
            "       'OAuthApplicationId', 'FileName', 'FolderPath', 'ResourceIdName',\n",
            "       'OSFamily', 'OSVersion', 'CountryCode', 'State', 'City', 'day_of_week',\n",
            "       'EncodedIncidentGrade', 'EncodedCategory', 'EncodedMitreTechniques',\n",
            "       'EntityType_AmazonResource', 'EntityType_AzureResource',\n",
            "       'EntityType_Blob', 'EntityType_BlobContainer',\n",
            "       'EntityType_CloudApplication', 'EntityType_CloudLogonRequest',\n",
            "       'EntityType_CloudLogonSession', 'EntityType_Container',\n",
            "       'EntityType_ContainerImage', 'EntityType_ContainerRegistry',\n",
            "       'EntityType_File', 'EntityType_GenericEntity',\n",
            "       'EntityType_GoogleCloudResource', 'EntityType_IoTDevice',\n",
            "       'EntityType_Ip', 'EntityType_KubernetesCluster',\n",
            "       'EntityType_KubernetesNamespace', 'EntityType_KubernetesPod',\n",
            "       'EntityType_Machine', 'EntityType_MailCluster',\n",
            "       'EntityType_MailMessage', 'EntityType_Mailbox',\n",
            "       'EntityType_MailboxConfiguration', 'EntityType_Malware',\n",
            "       'EntityType_Nic', 'EntityType_OAuthApplication', 'EntityType_Process',\n",
            "       'EntityType_RegistryKey', 'EntityType_RegistryValue',\n",
            "       'EntityType_SecurityGroup', 'EntityType_Url', 'EntityType_User',\n",
            "       'EncodedEvidenceRole', 'EncodedSuspicionLevel', 'EncodedLastVerdict'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "expected_columns = df.columns\n",
        "print(expected_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XrC63JXLCjnI"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, X, y= train_test_split_custom(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5AbthPe5TSP"
      },
      "source": [
        "# --> Logistic Regression Model validation for Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwfGpFlQClxG",
        "outputId": "bd8f2f19-095f-48dc-82b8-9ea3be3f3b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6185\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.85      0.69    822245\n",
            "           1       0.57      0.20      0.29    404816\n",
            "           2       0.71      0.59      0.65    659022\n",
            "\n",
            "    accuracy                           0.62   1886083\n",
            "   macro avg       0.62      0.55      0.54   1886083\n",
            "weighted avg       0.63      0.62      0.59   1886083\n",
            "\n",
            "0.6184876275328286\n"
          ]
        }
      ],
      "source": [
        "print(logistic_regression_model(X_train, X_test, y_train, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EayPTIk5Zs3"
      },
      "source": [
        "# -->  XGBoost Model validation for Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Y1B5tPCnrM",
        "outputId": "12d02d9b-27ba-45ff-a74d-908711905518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9150\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92    822245\n",
            "           1       0.93      0.84      0.88    404816\n",
            "           2       0.94      0.91      0.93    659022\n",
            "\n",
            "    accuracy                           0.91   1886083\n",
            "   macro avg       0.92      0.90      0.91   1886083\n",
            "weighted avg       0.92      0.91      0.91   1886083\n",
            "\n",
            "0.9149793513859146\n"
          ]
        }
      ],
      "source": [
        "print(xgboost_model(X_train, X_test, y_train, y_test, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtmdLNTT5juX"
      },
      "source": [
        "# --> Random Forest Model validation for Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PibNErt7Cpcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fffca3b-ee57-4777-bb78-bc61a19afc75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9764\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98    822245\n",
            "           1       0.97      0.96      0.97    404816\n",
            "           2       0.99      0.97      0.98    659022\n",
            "\n",
            "    accuracy                           0.98   1886083\n",
            "   macro avg       0.98      0.97      0.97   1886083\n",
            "weighted avg       0.98      0.98      0.98   1886083\n",
            "\n",
            "0.9763843902945947\n"
          ]
        }
      ],
      "source": [
        "print(random_forest_model(X_train, X_test, y_train, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e3REN3Qo_et",
        "outputId": "263302e6-b8a6-4a0e-feb3-c30384488b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold Accuracy: 0.9153\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92    822136\n",
            "           1       0.93      0.84      0.88    405432\n",
            "           2       0.94      0.91      0.93    658515\n",
            "\n",
            "    accuracy                           0.92   1886083\n",
            "   macro avg       0.92      0.90      0.91   1886083\n",
            "weighted avg       0.92      0.92      0.91   1886083\n",
            "\n",
            "Fold Accuracy: 0.9146\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92    822136\n",
            "           1       0.93      0.85      0.89    405432\n",
            "           2       0.94      0.91      0.92    658515\n",
            "\n",
            "    accuracy                           0.91   1886083\n",
            "   macro avg       0.92      0.90      0.91   1886083\n",
            "weighted avg       0.92      0.91      0.91   1886083\n",
            "\n",
            "Fold Accuracy: 0.9159\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92    822136\n",
            "           1       0.93      0.84      0.88    405432\n",
            "           2       0.94      0.91      0.93    658515\n",
            "\n",
            "    accuracy                           0.92   1886083\n",
            "   macro avg       0.92      0.90      0.91   1886083\n",
            "weighted avg       0.92      0.92      0.92   1886083\n",
            "\n",
            "Fold Accuracy: 0.9146\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92    822136\n",
            "           1       0.93      0.84      0.88    405432\n",
            "           2       0.94      0.91      0.92    658515\n",
            "\n",
            "    accuracy                           0.91   1886083\n",
            "   macro avg       0.92      0.90      0.91   1886083\n",
            "weighted avg       0.92      0.91      0.91   1886083\n",
            "\n",
            "Fold Accuracy: 0.9145\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92    822135\n",
            "           1       0.93      0.84      0.88    405433\n",
            "           2       0.94      0.91      0.92    658514\n",
            "\n",
            "    accuracy                           0.91   1886082\n",
            "   macro avg       0.92      0.90      0.91   1886082\n",
            "weighted avg       0.92      0.91      0.91   1886082\n",
            "\n",
            "\n",
            "Average Accuracy across 5 folds: 0.9150\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(kfold_xgboost(X, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCoXVQHm5pvH"
      },
      "source": [
        "# **Testing Data to test the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Epjciizxpq0U"
      },
      "outputs": [],
      "source": [
        "# Read test data\n",
        "\n",
        "def read_test_data_custom():\n",
        "  # Reading from Goggle drive as running millions of records in local is not possible\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  # Reading data from drive\n",
        "\n",
        "  # Reading 0.01 percent of data inorder to make this test with smaller data before moving to whole data\n",
        "  #nd_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_data/GUIDE_Test.csv', skiprows=lambda i: i>0 and np.random.rand() > 0.01)\n",
        "\n",
        "  # Once everything done, comment above line and run below\n",
        "  nd_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_data/GUIDE_Test.csv')\n",
        "  return nd_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kts6l4QXs6sr",
        "outputId": "be660669-4f03-4924-f5fd-e6721e62fa60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-3619b03206da>:13: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  nd_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_data/GUIDE_Test.csv')\n"
          ]
        }
      ],
      "source": [
        "test_df = read_test_data_custom()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry_ZKm8KvNNQ",
        "outputId": "1f5474c5-9ccd-49a4-c174-a7ba6cba6f4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4147784, 46)\n"
          ]
        }
      ],
      "source": [
        "test_df = data_quality(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "070zc2-XvVYZ"
      },
      "outputs": [],
      "source": [
        "test_df = data_cleaning(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5J2MwxbgvY33"
      },
      "outputs": [],
      "source": [
        "test_df = encode_categorical_values(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WV4pJQRAznCT"
      },
      "outputs": [],
      "source": [
        "for col in expected_columns:\n",
        "  if col not in test_df.columns:\n",
        "    test_df[col] = 0\n",
        "\n",
        "test_df = test_df[expected_columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9qen7M-6GmE"
      },
      "source": [
        "# **Logistic Regression Model for Test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wrf8AtZcxWqI"
      },
      "outputs": [],
      "source": [
        "def test_logistic_regression_model(test_df):\n",
        "\n",
        "  # Load the trained scaler\n",
        "  with open(\"log_scaler.pkl\", \"rb\") as scaler_file:\n",
        "      scaler = pickle.load(scaler_file)\n",
        "\n",
        "  X_test_new = test_df.drop(columns=['EncodedIncidentGrade'])\n",
        "\n",
        "  X_test_scaled = scaler.transform(X_test_new)\n",
        "\n",
        "  with open(\"logistic_regression_model.pkl\", \"rb\") as model_file:\n",
        "    xgb_model = pickle.load(model_file)\n",
        "\n",
        "  y_pred = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "  y_test_new = test_df['EncodedIncidentGrade']\n",
        "  print(f\"Accuracy: {accuracy_score(y_test_new, y_pred):.4f}\")\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test_new, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6uQ9KWL0Pxz",
        "outputId": "e681d6e5-3c6e-4aa2-84e3-9a99d99f44cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6194\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.87      0.69   1752850\n",
            "           1       0.62      0.11      0.19    902630\n",
            "           2       0.72      0.64      0.68   1492304\n",
            "\n",
            "    accuracy                           0.62   4147784\n",
            "   macro avg       0.64      0.54      0.52   4147784\n",
            "weighted avg       0.63      0.62      0.58   4147784\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(test_logistic_regression_model(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfENMz5H6NCy"
      },
      "source": [
        "# **XGBoost Model for Test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KDk2bO3X0FDQ"
      },
      "outputs": [],
      "source": [
        "def test_xgboost_model(test_df):\n",
        "\n",
        "  # Load the trained scaler\n",
        "  with open(\"xgb_scaler.pkl\", \"rb\") as scaler_file:\n",
        "      scaler = pickle.load(scaler_file)\n",
        "\n",
        "  X_test_new = test_df.drop(columns=['EncodedIncidentGrade'])\n",
        "  X_test_scaled = scaler.transform(X_test_new)\n",
        "  with open(\"xgboost_model.pkl\", \"rb\") as model_file:\n",
        "    xgb_model = pickle.load(model_file)\n",
        "\n",
        "  y_pred = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "  y_test_new = test_df['EncodedIncidentGrade']\n",
        "  print(f\"Accuracy: {accuracy_score(y_test_new, y_pred):.4f}\")\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test_new, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl3vglGP066c",
        "outputId": "1dde4056-74db-45d4-c3b3-0b988c55f701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8822\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.91      0.89   1752850\n",
            "           1       0.86      0.79      0.83    902630\n",
            "           2       0.90      0.90      0.90   1492304\n",
            "\n",
            "    accuracy                           0.88   4147784\n",
            "   macro avg       0.88      0.87      0.87   4147784\n",
            "weighted avg       0.88      0.88      0.88   4147784\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(test_xgboost_model(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLYXgKqw6T3_"
      },
      "source": [
        "# **Random Forest Model for Test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5eh0TLOj0_QV"
      },
      "outputs": [],
      "source": [
        "def test_random_forest_model(test_df):\n",
        "\n",
        "  # Load the trained scaler\n",
        "  with open(\"rf_scaler.pkl\", \"rb\") as scaler_file:\n",
        "      scaler = pickle.load(scaler_file)\n",
        "\n",
        "  X_test_new = test_df.drop(columns=['EncodedIncidentGrade'])\n",
        "  X_test_scaled = scaler.transform(X_test_new)\n",
        "  with open(\"random_forest_model.pkl\", \"rb\") as model_file:\n",
        "    xgb_model = pickle.load(model_file)\n",
        "\n",
        "  y_pred = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "  y_test_new = test_df['EncodedIncidentGrade']\n",
        "  print(f\"Accuracy: {accuracy_score(y_test_new, y_pred):.4f}\")\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test_new, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0BP5Xyg288X",
        "outputId": "72b5f8eb-5476-42bc-e41e-13040d0c2f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9134\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92   1752850\n",
            "           1       0.92      0.84      0.88    902630\n",
            "           2       0.94      0.92      0.93   1492304\n",
            "\n",
            "    accuracy                           0.91   4147784\n",
            "   macro avg       0.92      0.90      0.91   4147784\n",
            "weighted avg       0.91      0.91      0.91   4147784\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(test_random_forest_model(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D25Tq-WT3Erb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}